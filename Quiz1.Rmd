---
title: 'Quiz 1'
author: "Luke Flyer"
output: pdf_document
---

```{r}

library(knitr)
library(magrittr)
library(kableExtra)
```

## Question 1

Which member of the teaching team has office hours Tuesdays 10:30 - 11:30am?

According to the syllabus, there are no members of the teaching team who have office hours during that time.

## Question 2

If you have a question about the course content, how should you ask it? Select all that apply. a. Ask in office hours b. Post in Slack c. Ask in Slack

As we do no use Slack for this course, "Ask in office hours" seems to be the only applicable option.

## Question 3

If a homework assignment is due on Wednesday at 11:59pm, when is the latest it can be turned in for credit?

There are no extensions on any assignments, so the latest it can be turned in for credit is 11:59pm on Wednesday.

## Question 4

The course policy on the use of artificial intelligence is based on two guiding principles: (1) Cognitive dimension and (2) Ethical dimension. Briefly describe what each dimension means.

Cognitive dimension: AI should only be used as a tool to facilitate learning, not take the place of it. It should not reduce the ability to think clearly. Ethical dimension: Use of AI should be transparent (explicitly cited) and only be used for code and not narrative.

## Question 5

Suppose data from 100 randomly selected elementary school students are used to fit a regression model to predict height (in inches) based on age (in months). Which of the following best describes the normality assumption for this model?

Height (in inches) is normally distributed at any given age (in months).

## Question 6a

Let ùëå ‚àº Binomial(ùëõ, ùúÉ). Write the likelihood of the Binomial distribution and define it to be ùêø(ùúÉ), where ùëõ is known and ùúÉ is unknown.

$$
L(\theta) = \prod_{i=1}^{m} P(Y_i = r_i | n, \theta)
$$

where $r_i$ is the number of successes in the i-th observation. From the PMF of Y, we can then say each observation will have the distribution:

$$
\binom{n}{r_i} \theta^{r_i} (1 - \theta)^{n - r_i}
$$

Plugging this back into the likelihood formula, we get:

$$
L(\theta) = \prod_{i=1}^{m} \binom{n}{r_i} \theta^{r_i} (1 - \theta)^{n - r_i}
$$

## Question 6b

Simplifying the likelihood function, we get

$$
L(\theta) = \left( \prod_{i=1}^{m} \binom{n}{r_i} \right) \theta^{\sum_{i=1}^{m} r_i} (1 - \theta)^{\sum_{i=1}^{m} (n - r_i)}
$$

Taking the natural log of the function, we get the following:

$$
\ell(\theta) = \log L(\theta) = \log \left( \left( \prod_{i=1}^{m} \binom{n}{r_i} \right) \theta^{\sum_{i=1}^{m} r_i} (1 - \theta)^{\sum_{i=1}^{m} (n - r_i)} \right)
$$

$$
\ell(\theta) = \log \left( \prod_{i=1}^{m} \binom{n}{r_i} \right) + \sum_{i=1}^{m} r_i \log(\theta) + \sum_{i=1}^{m} (n - r_i) \log(1 - \theta)
$$

The first term does not depend on $\theta$, so we can ignore it when taking derivative, giving us the following:

$$
\ell(\theta) = \sum_{i=1}^{m} r_i \log(\theta) + \sum_{i=1}^{m} (n - r_i) \log(1 - \theta)
$$

$$
\frac{d\ell(\theta)}{d\theta} = \frac{\sum_{i=1}^{m} r_i}{\theta} - \frac{\sum_{i=1}^{m} (n - r_i)}{1 - \theta}
$$

We can now set the derivative equal to 0 and solve for $\theta$:

$$
\frac{\sum_{i=1}^{m} r_i}{\theta} - \frac{\sum_{i=1}^{m} (n - r_i)}{1 - \theta} = 0
$$

$$
\left( \sum_{i=1}^{m} r_i \right) (1 - \theta) = \left( \sum_{i=1}^{m} (n - r_i) \right) \theta
$$

$$
\sum_{i=1}^{m} r_i - \sum_{i=1}^{m} r_i \theta = \sum_{i=1}^{m} (n - r_i) \theta
$$

$$
\sum_{i=1}^{m} r_i = \theta \left( \sum_{i=1}^{m} r_i + \sum_{i=1}^{m} (n - r_i) \right)
$$

$$
\sum_{i=1}^{m} r_i = \theta \left( \sum_{i=1}^{m} r_i + m n - \sum_{i=1}^{m} r_i \right)
$$

$$
\sum_{i=1}^{m} r_i = \theta \cdot m n
$$

$$
\hat{\theta}_{MLE} = \frac{\sum_{i=1}^{m} r_i}{m n}
$$

Now, we must test the second derivative to confirm the critical point as a maximum:

$$
\frac{d^2 \ell(\theta)}{d\theta^2} = -\frac{\sum_{i=1}^{m} r_i}{\theta^2} - \frac{\sum_{i=1}^{m} (n - r_i)}{(1 - \theta)^2}
$$

For all $\theta$ between 0 and 1, this second derivative will be negative, confirming the above critical point as a maximum. In words, this MLE is equal to the sample proportion of successes, with the total number of successes in the numerator and the total number of trials in the denominator.

## Question 7

Provide the three assumptions of a generalized linear model that may deviate from a linear model. State the one assumption they share in common.

Assumptions that deviate from a linear model:

1.  Linearity: the relationship between the response variable and the predictor variable(s) can be non-linear. So far, we have seen how this can be useful for situations like Poisson regression, where there is not a strictly linear relationship.
2.  Normality: The response variable can be non-normal when using a GLM. Again this is useful in situations like Poisson regression, where the response variable will not follow a normal distribution.
3.  Constant variance: Variance in the response variable is allowed to differ at different levels of the predictor. This allows GLMs to handle various situations where the variance in the outcome will not be constant along the predictor variables.

Assumption in common:

1.  Independence: Each observation in both a GLM and a linear model must be independent. Knowing the distance from the regression line for one point should not provide any information about another.

## Question 8

Describe, in your own words, the importance of exploratory data analysis (in less than two sentences).

Before fitting any models, it is important to understand our data, specifically focusing on the distributions of our variables and the relationships between the variables. Visualizing, graphically and numerically, the variables and their correlations will help us decide appropriate modeling techniques to use.

## Question 9

Define the maximum likelihood estimator. Is it always optimal or the best estimator that we should consider; explain.

The maximum likelihood estimate is the value of a parameter (among all possible values of the parameter) at which we are most likely to observe the data. Mathematically, the MLE is the value of $\theta$ that maximizes the likelihood function $L(\theta)$ within its domain. Though MLEs are intuitively the "best" estimator to consider and quite often technically the best as well, they are not always optimal. The desirable charactaristics of an MLE tend to hold when the sample size is large enough. As the sample size increases, MLEs converge to the true paramter value and they become increasingly more normal. Without a large enough sample size, however, MLEs can prove to be biased or unreliable. Bootstrapping or Bayesian methods are possible alternatives to MLE when the conditions aren't met, but MLE is still highly and commonly useful.

## Question 10

Define a likelihood using words (and without notation). Can we make probability statements regarding the likelihood? Explain.

Likelihoods are measures of how likely we are to observe a given set of data for a specific parameter (or set of parameters). Essentially, we are given some observed data, we have some estimate for the parameters in a model, and a likelihood will tell us how likely we are to observe that data for those parameter estimates.

We cannot make probability statements regarding the likelihood in the strict sense that we tend to use probability statements. A likelihood function inputs a parameter value and outputs a li kelihood of seeing the observed data at that parameter value. A probability function inputs observed data and outputs the probability of observing that outcome. This difference is subtle, but important. Another important difference is that probability distributions (whether discrete or continuous) will sum or integrate to 1, but this is not the case with likelihoods. This difference has the implication that we cannot make any statements involving "chance" since likelihood functions can only compare likelihoods (not outcomes) of parameter values, whereas probability functions can compare specific probabilities of outcomes against each other.
