---
title: 'Quiz 3'
author: "Luke Flyer"
output: pdf_document
---

```{r}

library(knitr)
library(magrittr)
library(kableExtra)
```

## Question 1

It seems like it would be acceptable to use a quasiPoisson model even when the data does not exhibit overdispersion. Of course, it would certainly over-complicate things by adding the extra dispersion parameter, but the resulting model would still be valid. We would expect the dispersion parameter to be equal to 1.0 if the data follows a Poisson distribution (mean = variance).

## Question 2

The support of this distribution if {1, 2, 3, ...}, or the set of all integers greater than or equal to one. This means that $Y$ can take on only these possible values.

## Question 3

Start with the PMF:

$$
P(Y = y) = \frac{e^{-\lambda} \lambda^y}{(1 - e^{-\lambda}) y!}
$$

We want to express this distribution in the canonical form, which is:

$$
f(y | \lambda) = h(y) \exp \left( \eta(\lambda) T(y) - \psi(\lambda) \right)
$$

Manipulating the original function:

$$
P(Y = y) = \frac{1}{y!} \cdot \frac{e^{-\lambda} \lambda^y}{1 - e^{-\lambda}}
$$

$$
P(Y = y) = \frac{1}{y!} \cdot \frac{e^{-\lambda} e^{y \log(\lambda)}}{1 - e^{-\lambda}}
$$

$$
P(Y = y) = \frac{1}{y!} \cdot \exp\left( -\lambda + y \log(\lambda) \right) \cdot \frac{1}{1 - e^{-\lambda}}
$$

$$
P(Y = y) = \frac{1}{y!} \cdot \exp\left( -\lambda + y \log(\lambda) - \log(1 - e^{-\lambda}) \right)
$$

Now we have the canonical form:

$$
P(Y = y) = \frac{1}{y!} \cdot \exp\left( y \log(\lambda) - \log(1 - e^{-\lambda}) - \lambda \right)
$$

where:

$$
h(y) = \frac{1}{y!}, \quad \eta(\lambda) = \log(\lambda), \quad T(y) = y, \quad \psi(\lambda) = \log(1 - e^{-\lambda}) + \lambda
$$

## Question 4

We start with the log-partition function for the zero-truncated Poisson distribution:

$$
\psi(\lambda) = \log(1 - e^{-\lambda}) + \lambda
$$

The canonical parameter is $\eta(\lambda) = \log(\lambda)$, so $\lambda = e^{\eta(\lambda)}$.

To compute the expected value $\mathbb{E}[Y]$, we take the derivative of $\psi(\lambda)$ with respect to $\eta(\lambda)$:

$$
\mathbb{E}[Y] = \frac{d}{d\eta(\lambda)} \psi(\lambda)
$$

We know this is the case because we know that the first and second derivatives of $\psi(\lambda)$ are the mean and variances, respectively, of the sufficient statistic $T(y)$. Since we know that $T(y) = y$, we can then say that the first and second derivatives of $\psi(\lambda)$ are the mean and variances, respectively, of $Y$. This is a convenient method of finding the mean when $T(y) = y$.

For the calculation, we start by using the chain rule:

$$
\mathbb{E}[Y] = \frac{d}{d\lambda} \psi(\lambda) \cdot \frac{d\lambda}{d\eta(\lambda)}
$$

We already know that $\frac{d\lambda}{d\eta(\lambda)} = \lambda$ because: $\frac{d}{d\eta(\lambda)} e^{\eta(\lambda)} = e^{\eta(\lambda)} = \lambda$.

Now, compute $\frac{d}{d\lambda} \psi(\lambda)$:

$$
\frac{d}{d\lambda} \psi(\lambda) = \frac{e^{-\lambda}}{1 - e^{-\lambda}} + 1
$$

Now combine the derivative with the chain rule result:

$$
\mathbb{E}[Y] = \lambda \left( \frac{e^{-\lambda}}{1 - e^{-\lambda}} + 1 \right)
$$

Simplify:

$$
\mathbb{E}[Y] = \lambda \cdot \frac{1}{1 - e^{-\lambda}}
$$

and we can rearrange to get the following result:

$$
\mathbb{E}[Y] = \frac{\lambda e^{\lambda}}{e^{\lambda} - 1}
$$

## Question 5

The canonical link is as follows:

$$\log(\lambda) = X_i^T \beta$$
We know this because the canonical parameter (taken from the function written in canonical form) is:

$$\quad \eta(\lambda) = \log(\lambda)$$

And we know that we can use this canonical parameter to derive the canonical link that will model the relationship between the predictors, $X \beta$, and the expected count.

## Question 6

We just defined the canonical link for the distribution, which is the logarithm of the rate parameter $\lambda$:

$$
\log(\lambda) = X_i^T \beta
$$

Exponentiate both sides:

$$
\lambda = \exp(X_i^T \beta)
$$

The likelihood function for this distribution is as follows (plugging in $\lambda = \exp(X_i^T \beta)$):

$$
\mathcal{L}(\beta) = \prod_{i=1}^n \frac{e^{-\exp(X_i^T \beta)} (\exp(X_i^T \beta))^{y_i}}{(1 - e^{-\exp(X_i^T \beta)}) y_i!}
$$

Now, we take the log of both sides to get the log-likelihood:

$$
\log \mathcal{L}(\beta) = \sum_{i=1}^n \left[ -\exp(X_i^T \beta) + y_i \log(\exp(X_i^T \beta)) - \log(y_i!) - \log(1 - e^{-\exp(X_i^T \beta)}) \right]
$$

Simplifying $\log(\exp(X_i^T \beta)) = X_i^T \beta$, we get:

$$
\log \mathcal{L}(\beta) = \sum_{i=1}^n \left[ y_i X_i^T \beta - \exp(X_i^T \beta) - \log(y_i!) - \log(1 - e^{-\exp(X_i^T \beta)}) \right]
$$

## Question 7

We already have the log-likelihood function for the zero-truncated Poisson distribution:

$$
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \mathbf{X}_i^\top \boldsymbol{\beta} - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) - \log(y_i!) - \log \left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right) \right]
$$

Differentiating the first term:

$$
\frac{\partial}{\partial \boldsymbol{\beta}} \left( y_i \mathbf{X}_i^\top \boldsymbol{\beta} \right) = y_i \mathbf{X}_i
$$

Differentiating the second term:

$$
\frac{\partial}{\partial \boldsymbol{\beta}} \left( -\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \right) = -\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i
$$

Differentiating the third term:

The third term $-\log(y_i!)$ does not depend on $\boldsymbol{\beta}$, so its derivative is zero.

Differentiating the fourth term, using the chain rule to do so:

$$
\frac{\partial}{\partial \boldsymbol{\beta}} \left( -\log \left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right) \right)
$$

The derivative of the logarithmic part is:

$$
\frac{1}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))}
$$

Now, differentiate the inner term $-\exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))$ with respect to $\boldsymbol{\beta}$, and put it in numerator (multiply by expression above):

$$
\frac{\exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))}
$$

Now, we have to combine all terms to get the score function, which is the sum of the derivatives:

$$
\nabla_{\boldsymbol{\beta}} \log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \mathbf{X}_i - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i - \frac{\exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} \right]
$$

We factor out the common term $\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i$ from the second and third terms: $$
\nabla_{\boldsymbol{\beta}} \log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \mathbf{X}_i - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \left( 1 + \frac{\exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} \right) \right]
$$

Simplifying the expression inside the parentheses: $$
1 + \frac{\exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} = \frac{1}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))}
$$

Now we have our final score function which is: $$
\nabla_{\boldsymbol{\beta}} \log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left[ y_i \mathbf{X}_i - \frac{\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} \right]
$$

Now, we can take another derivative to get the Hessian. The first term in the score function is $y_i \mathbf{X}_i$, so does not depend on $\boldsymbol{\beta}$. Therefore, its derivative with respect to $\boldsymbol{\beta}$ is zero:

$$
\frac{d}{d\boldsymbol{\beta}} \left( y_i \mathbf{X}_i \right) = 0
$$

For the second term in the score function we need to use the quotient rule. Define the following

$$
f(\boldsymbol{\beta}) = \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i
$$ $$
g(\boldsymbol{\beta}) = 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))
$$

We will apply the quotient rule to get the second derivative:

$$
\frac{d}{d\boldsymbol{\beta}} \left( \frac{f(\boldsymbol{\beta})}{g(\boldsymbol{\beta})} \right) = \frac{g(\boldsymbol{\beta}) \frac{d}{d\boldsymbol{\beta}} f(\boldsymbol{\beta}) - f(\boldsymbol{\beta}) \frac{d}{d\boldsymbol{\beta}} g(\boldsymbol{\beta})}{g(\boldsymbol{\beta})^2}
$$

The derivative of $f(\boldsymbol{\beta})$:

$$
\frac{d}{d\boldsymbol{\beta}} f(\boldsymbol{\beta}) = \frac{d}{d\boldsymbol{\beta}} \left( \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \right) = \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \mathbf{X}_i^\top
$$

The derivative of $g(\boldsymbol{\beta})$:

$$
\frac{d}{d\boldsymbol{\beta}} g(\boldsymbol{\beta}) = \frac{d}{d\boldsymbol{\beta}} \left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right) = \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i
$$

Now, plug into the quotient rule:

$$
\frac{d}{d\boldsymbol{\beta}} \left( \frac{\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} \right)
=
$$

$$
\frac{(1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \mathbf{X}_i^\top - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \cdot \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{(1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})))^2}
$$

Unsure if this can be simplified further, we now have our Hessian:

$$
\nabla_{\boldsymbol{\beta}}^2 \log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \frac{\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \mathbf{X}_i^\top \left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right) - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \cdot \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{\left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right)^2}
$$

## Question 8

The Newton-Raphson updating rule is as follows:

$$
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \nabla^2_{\boldsymbol{\beta}} \log L(\boldsymbol{\beta}) \right]^{-1} \nabla_{\boldsymbol{\beta}} \log L(\boldsymbol{\beta})
$$

Now we can plug in both the score function and the Hessian, and we can have the full Newton-Raphson update rule:

$$\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \left[ \sum_{i=1}^{n} \frac{\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \mathbf{X}_i^\top \left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right) - \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i \cdot \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \cdot \exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{\left( 1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta})) \right)^2}\right]^{-1}$$

$$
.\sum_{i=1}^{n} \left[ y_i \mathbf{X}_i - \frac{\exp(\mathbf{X}_i^\top \boldsymbol{\beta}) \mathbf{X}_i}{1 - \exp(-\exp(\mathbf{X}_i^\top \boldsymbol{\beta}))} \right]
$$

\*Note that the above equation is written on 2 lines, as it would not fit into 1.

## Question 9

The name is the Iteratively Re-weighted Least Squares algorithm.

## Question 10

No.
